

# üß† Medical Chatbot using LangChain, Pinecone, and Groq LLM

A powerful AI-powered chatbot designed to answer medical queries using real-time vector search and LLM-based responses. Built using **Flask**, **LangChain**, **Pinecone**, and **Groq API**, and deployed for public access via **Render**.

## üìå Live Demo

üîó [Try the Chatbot Here](https://healthai-amla.onrender.com)  


---

## üí° Project Overview

This chatbot answers user queries based on a medical knowledge base using a Retrieval-Augmented Generation (RAG) architecture. It combines document similarity search via **Pinecone** and intelligent response generation using **Groq‚Äôs LLaMA model**.

---

## üîß Features

‚úÖ Real-time chatbot interface using HTML, CSS, and JavaScript  
‚úÖ Server-side logic using Flask and Python  
‚úÖ RAG pipeline using LangChain: combines retrieval and LLM response  
‚úÖ Uses **Pinecone vector DB** for semantic document search  
‚úÖ Uses **Groq‚Äôs hosted LLaMA 4 model** via API for fast and intelligent replies  
‚úÖ Responsive and styled chat UI  
‚úÖ Fully deployed on **Render** with a public link

---

## üõ†Ô∏è Tech Stack

| Category       | Technology                     |
|----------------|--------------------------------|
| Frontend       | HTML, CSS (Bootstrap, jQuery)  |
| Backend        | Python, Flask                  |
| LLM            | Groq API (LLaMA 4)             |
| Vector DB      | Pinecone                       |
| Retrieval/RAG  | LangChain                      |
| Deployment     | Render                         |
| Environment    | `.env` for API keys            |

---

## üß† Architecture

```

User ‚ûù Flask App ‚ûù LangChain ‚ûù
‚ûù Pinecone (Retrieval)
‚ûù Groq API (LLM Response)
‚ûù Final Answer ‚ûù Chat UI

```

---

## üß™ How I Set Up Pinecone

1. Created a **Pinecone account** and a new index named `medicalbot`
2. Used Hugging Face's `sentence-transformers/all-MiniLM-L6-v2` to generate vector embeddings
3. Uploaded text/document chunks to the Pinecone index
4. Integrated Pinecone with LangChain's `PineconeVectorStore` to retrieve top 3 relevant documents for any user query

---

## ü§ñ How I Integrated Groq LLM

1. Generated a **Groq API key**
2. Created a custom `GroqChatLLM` class extending `langchain.llms.base.LLM`
3. Used Groq‚Äôs hosted LLaMA 4 Scout model via REST API
4. Combined with LangChain‚Äôs prompt and retriever in a `RetrievalChain`

---

## üìÅ Folder Structure

```

‚îú‚îÄ‚îÄ app.py                # Main Flask app
‚îú‚îÄ‚îÄ requirements.txt      # All dependencies
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ chat.html         # Chatbot frontend
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ style.css         # Styling
‚îú‚îÄ‚îÄ .env                  # Pinecone and Groq API keys (not pushed)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ helper.py         # Embedding utility

````

---

## üöÄ Deployment on Render

1. Pushed project to GitHub
2. Connected GitHub repo to Render
3. Set the `Start Command` as: `gunicorn app:app`
4. Set environment variables for:
   - `PINECONE_API_KEY`
   - `GROQ_API_KEY`
5. Render builds and exposes a public URL

---

## üìå Environment Variables (`.env`)

```env
PINECONE_API_KEY=your_pinecone_key_here
GROQ_API_KEY=your_groq_key_here
````

*(These are set securely in Render's dashboard and not pushed to GitHub)*

---

## üß† Future Improvements

* Upload PDF documents to index dynamically
* Add user authentication
* Expand to other domains (e.g., legal, education)
* Analytics dashboard for user questions

---

## üôã‚Äç‚ôÇÔ∏è About Me

I‚Äôm IT student passionate about AI and real-world applications of LLMs. This project reflects my hands-on learning of AI pipelines, LangChain, and cloud deployment. Open to internships and AI-focused opportunities.

---

## üì¨ Contact

üìß [atharvnikam38@gmail.com](mailto:atharvnikam38@gmail.com)
üêô [GitHub](https://github.com/atharvnikam38)

---

> If you like this project, feel free to ‚≠ê the repo!





